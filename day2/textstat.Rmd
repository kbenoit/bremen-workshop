---
title: "Computing textual statistics"
output: github_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##",
  fig.path = "images/"
)
```

```{r}
library("quanteda")
```

## Frequency analysis

### Exercise

Here we will work with a large corpus of articles from the Guardian newspaper.

1. Use `data_corpus_guardian` from the **quanteda.corpora** package.

```{r}
data_corpus_guardian <- quanteda.corpora::download('data_corpus_guardian')
```

2. Create a `tokens` object and remove stopwords and punctuation.

```{r}

```


3. Keep only the term "Brexit*" and a window of 5 words.

Hint: Use `tokens_keep()`.

```{r}

```


4. Which terms co-occur most often with Brexit? (`textstat_frequency()` or `topfeatures()`?


## Readability and lexical diversity

### Exercise

1. Load the Irish budget speeches

```{r}
data(data_corpus_irishbudget2010, package = "quanteda.textmodels")
```

2. Compute the Flesch-Kincaid measure for each speech.  Which were the highest and lowest speeches on this measure?

```{r}

```


3. Compute the lexical diversity using the MATTR measure.  Which were the highest and lowest speeches on this measure?

```{r}

```

